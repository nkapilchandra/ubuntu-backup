{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialization\n",
    "- Cost Computation\n",
    "- Forward Propagation\n",
    "- Backward Propagation\n",
    "- Model creation\n",
    "- Update parameters\n",
    "- Prediction\n",
    "- Gradient Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import h5py\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_params(layer_dims):\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    l = len(layer_dims)\n",
    "    \n",
    "    for i in range(1,l):\n",
    "        parameters['W' + str(i)] = np.random.randn(layer_dims[i],layer_dims[i-1])/np.sqrt(layer_dims[i-1]) #*0.01\n",
    "        parameters['b' + str(i)] = np.zeros((layer_dims[i],1))\n",
    "        \n",
    "        assert(parameters['W' + str(i)].shape == (layer_dims[i],layer_dims[i-1]))\n",
    "        assert(parameters['b' + str(i)].shape == (layer_dims[i],1))\n",
    "    \n",
    "    return parameters\n",
    "        \n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1[[ 0.93781623 -0.35319773 -0.3049401 ]\n",
      " [-0.61947872  0.49964333 -1.32879399]]\n",
      "b1[[ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Check the functionality of the above function\n",
    "params = initialize_params([3,2,1])\n",
    "print('W1' + str(params['W1']))\n",
    "print('b1' + str(params['b1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return (1/(1+np.exp(-z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return (z + np.absolute(z))/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Prop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(A_prev,W,b,activation):\n",
    "    Z = np.dot(W,A_prev) + b\n",
    "\n",
    "    if activation == 'sigmoid':\n",
    "        A = sigmoid(Z)\n",
    "    elif activation == 'relu':\n",
    "        A = relu(Z)\n",
    "    elif activation == 'tanh':\n",
    "        A = np.tanh(Z)\n",
    "        \n",
    "    assert(A.shape == (W.shape[0],A_prev.shape[1]))\n",
    "    \n",
    "    cache = (Z,A_prev,W,b)\n",
    "    \n",
    "    return A,cache\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[ 0.96890023  0.11013289]]\n",
      "With ReLU: A = [[ 3.43896134  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Check the functionality of the above function\n",
    "A_prev = np.array([[-0.41675785,-0.05626683],[-2.1361961,1.64027081],[-1.79343559,-0.84174737]])\n",
    "W = np.array([[ 0.50288142,-1.24528809,-1.05795222]])\n",
    "b = np.array([[-0.90900761]])\n",
    "\n",
    "A, linear_activation_cache = forward_propagation(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = forward_propagation(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "       \n",
    "<table style=\"width:35%\">\n",
    "  <tr>\n",
    "    <td> **With sigmoid: A ** </td>\n",
    "    <td > [[ 0.96890023  0.11013289]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **With ReLU: A ** </td>\n",
    "    <td > [[ 3.43896131  0.        ]]</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_forward(X,parameters):\n",
    "    \n",
    "    L = len(parameters)//2\n",
    "    A = X\n",
    "    caches = []\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        A_prev = A\n",
    "        A,cache = forward_propagation(A_prev,parameters['W'+str(l)],parameters['b' + str(l)],'relu')\n",
    "        caches.append(cache)\n",
    "    \n",
    "    AL, cache = forward_propagation(A,parameters['W' + str(L)],parameters['b' + str(L)],'sigmoid')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "    \n",
    "    return AL, caches\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[ 0.03921668  0.70498921  0.19734387  0.04728177]]\n",
      "Length of caches list = 3\n"
     ]
    }
   ],
   "source": [
    "# Check the functionality of the above function\n",
    "X = np.array([[-0.31178367,0.72900392,0.21782079,-0.8990918 ],[-2.48678065, 0.91325152,1.12706373,-1.51409323],[ 1.63929108,-0.4298936,2.63128056,0.60182225],[-0.33588161,1.23773784,0.11112817,0.12915125],[ 0.07612761,-0.15512816,0.63422534 ,0.810655]])\n",
    "parameters = {'W3': np.array([[ 0.9398248 ,  0.42628539, -0.75815703]]), 'b3': np.array([[-0.16236698]]), 'W1': np.array([[ 0.35480861,  1.81259031, -1.3564758 , -0.46363197,  0.82465384],\n",
    "       [-1.17643148,  1.56448966,  0.71270509, -0.1810066 ,  0.53419953],\n",
    "       [-0.58661296, -1.48185327,  0.85724762,  0.94309899,  0.11444143],\n",
    "       [-0.02195668, -2.12714455, -0.83440747, -0.46550831,  0.23371059]]), 'b1': np.array([[ 1.38503523],\n",
    "       [-0.51962709],\n",
    "       [-0.78015214],\n",
    "       [ 0.95560959]]), 'b2': np.array([[ 1.50278553],\n",
    "       [-0.59545972],\n",
    "       [ 0.52834106]]), 'W2': np.array([[-0.12673638, -1.36861282,  1.21848065, -0.85750144],\n",
    "       [-0.56147088, -1.0335199 ,  0.35877096,  1.07368134],\n",
    "       [-0.37550472,  0.39636757, -0.47144628,  2.33660781]])}\n",
    "\n",
    "AL, caches = model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))\n",
    "# print(parameters['W3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "    <td> **AL** </td>\n",
    "    <td > [[ 0.03921668  0.70498921  0.19734387  0.04728177]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **Length of caches list ** </td>\n",
    "    <td > 3 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL,Y):\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = -1/m*np.sum((np.multiply(Y,np.log(AL)) + np.multiply(1-Y,np.log(1-AL))))\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.414931599615\n"
     ]
    }
   ],
   "source": [
    "# Check the functionality of the above function\n",
    "Y = np.array([[1,1,1]])\n",
    "AL = np.array([[ 0.8,0.9,0.4]])\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "\n",
    "    <tr>\n",
    "    <td>**cost** </td>\n",
    "    <td> 0.41493159961539694</td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(dA,Z):\n",
    "    \n",
    "    def relu_der(x):\n",
    "        k = np.copy(x)\n",
    "        np.putmask(k,k>=0,1)\n",
    "        np.putmask(k,k<0,0)\n",
    "        return k\n",
    "    \n",
    "    return np.multiply(dA,relu_der(Z))\n",
    "def sigmoid_backward(dA,Z):\n",
    "    \n",
    "    def sigmoid_der(x):\n",
    "        der = np.exp(-x)/(1+np.exp(-x))**2\n",
    "        return der\n",
    "    \n",
    "    return np.multiply(dA,sigmoid_der(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ,cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = 1/m*(np.dot(dZ,A_prev.T))\n",
    "    db = 1/m*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 0.51822968 -0.19517421]\n",
      " [-0.40506362  0.15255393]\n",
      " [ 2.37496825 -0.8944539 ]]\n",
      "dW = [[-0.10076895  1.40685096  1.64992504]]\n",
      "db = [[ 0.50629448]]\n"
     ]
    }
   ],
   "source": [
    "# Check the functionality of the above function\n",
    "dZ = np.array([[1.62434536,-0.61175641]])\n",
    "linear_cache = (np.array([[-0.52817175, -1.07296862],[ 0.86540763, -2.3015387 ],[ 1.74481176, -0.7612069 ]]),np.array([[ 0.3190391 , -0.24937038,  1.46210794]]),np.array([[-2.06014071]]))\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:90%\">\n",
    "  <tr>\n",
    "    <td> **dA_prev** </td>\n",
    "    <td > [[ 0.51822968 -0.19517421]\n",
    " [-0.40506361  0.15255393]\n",
    " [ 2.37496825 -0.89445391]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "        <td> **dW** </td>\n",
    "        <td > [[-0.10076895  1.40685096  1.64992505]] </td> \n",
    "    </tr> \n",
    "  \n",
    "    <tr>\n",
    "        <td> **db** </td>\n",
    "        <td> [[ 0.50629448]] </td> \n",
    "    </tr> \n",
    "    \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \n",
    "    Z, A_prev, W, b = cache\n",
    "#     A_prev, W, b, Z = cache\n",
    "    small_cache = (A_prev,W,b)\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        dZ = relu_backward(dA,Z)\n",
    "        dA_prev, dW, db = linear_backward(dZ,small_cache)\n",
    "    elif activation == 'sigmoid':\n",
    "        dZ = sigmoid_backward(dA,Z)\n",
    "        dA_prev, dW, db = linear_backward(dZ,small_cache)\n",
    "        \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.0110534 ]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576155]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989  0.        ]\n",
      " [ 0.37883606  0.        ]\n",
      " [-0.2298228  -0.        ]]\n",
      "dW = [[ 0.44513825  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "# Check the functionality of the above function\n",
    "linear_activation_cache = (np.array([[ 0.04153939, -1.11792545]]),np.array([[-2.1361961,1.64027081],[-1.79343559, -0.84174737],[ 0.50288142, -1.24528809]]),np.array([[-1.05795222, -0.90900761,  0.55145404]]),np.array([[ 2.29220801]]))\n",
    "AL = np.array([[-0.41675785,-0.05626683]])\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output with sigmoid:**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td > dA_prev </td> \n",
    "           <td >[[ 0.11017994  0.01105339]\n",
    " [ 0.09466817  0.00949723]\n",
    " [-0.05743092 -0.00576154]] </td> \n",
    "\n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > dW </td> \n",
    "           <td > [[ 0.10266786  0.09778551 -0.01968084]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > db </td> \n",
    "           <td > [[-0.05729622]] </td> \n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output with relu:**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td > dA_prev </td> \n",
    "           <td > [[ 0.44090989  0.        ]\n",
    " [ 0.37883606  0.        ]\n",
    " [-0.2298228   0.        ]] </td> \n",
    "\n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > dW </td> \n",
    "           <td > [[ 0.44513824  0.37371418 -0.10478989]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > db </td> \n",
    "           <td > [[-0.20837892]] </td> \n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_backward(AL, y, caches):\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    y = y.reshape(AL.shape)\n",
    "    \n",
    "    dAL = -(np.divide(y,AL) - np.divide(1-y,1-AL))\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache,'sigmoid')\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads['dA'+str(l+2)],current_cache,'relu')\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.52257901]\n",
      " [ 0.         -0.3269206 ]\n",
      " [ 0.         -0.32070404]\n",
      " [ 0.         -0.74079187]]\n"
     ]
    }
   ],
   "source": [
    "# Check the functionality of the above function\n",
    "AL = np.array([[ 1.78862847,0.43650985]])\n",
    "Y_assess = np.array([[1,0]])\n",
    "caches = (((np.array([[ 0.09649747, -1.8634927 ],[-0.2773882 , -0.35475898],[-0.08274148, -0.62700068],[-0.04381817, -0.47721803]]), np.array([[-1.31386475,  0.88462238,  0.88131804,  1.70957306],[ 0.05003364, -0.40467741, -0.54535995, -1.54647732],[ 0.98236743, -1.10106763, -1.18504653, -0.2056499 ]]), np.array([[ 1.48614836],[ 0.23671627],[-1.02378514]])),np.array([[-0.7129932,0.62524497],[-0.16051336, -0.76883635],[-0.23003072,  0.74505627]])), ((np.array([[ 1.97611078, -1.24412333],[-0.62641691, -0.80376609],[-2.41908317, -0.92379202]]), np.array([[-1.02387576,  1.12397796, -0.13191423]]), np.array([[-1.62328545]])), np.array([[ 0.64667545, -0.35627076]])))\n",
    "new_caches = []\n",
    "# new_caches.append((caches[0][0][0],caches[0][0][1],caches[0][0][2],caches[0][1][0]))\n",
    "# new_caches.append((caches[1][0][0],caches[1][0][1],caches[1][0][2],caches[0][1][0]))\n",
    "# new_caches = tuple(new_caches)\n",
    "\n",
    "cache1 = (np.array([[-0.7129932,0.62524497],[-0.16051336,-0.76883635],[-0.23003072,0.74505627]]),np.array([[ 0.09649747,-1.8634927 ],[-0.2773882,-0.35475898],[-0.08274148,-0.62700068],[-0.04381817,-0.47721803]]),np.array([[-1.31386475 ,0.88462238 ,0.88131804 ,1.70957306],[ 0.05003364,-0.40467741,-0.54535995,-1.54647732],[ 0.98236743,-1.10106763,-1.18504653,-0.2056499 ]]),np.array([[ 1.48614836],[ 0.23671627],[-1.02378514]]))\n",
    "cache2 = (np.array([[0.64667545,-0.35627076]]),np.array([[ 1.97611078,-1.24412333],[-0.62641691,-0.80376609],[-2.41908317,-0.92379202]]),np.array([[-1.02387576,1.12397796,-0.13191423]]),np.array([[-1.62328545]]))\n",
    "new_caches.append(cache1)\n",
    "new_caches.append(cache2)\n",
    "\n",
    "new_caches = tuple(new_caches)\n",
    "grads = model_backward(AL, Y_assess, new_caches)\n",
    "\n",
    "print(grads['dA1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters,grads,learning_rate):\n",
    "    \n",
    "    L = len(parameters)//2\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters['W'+str(l+1)] -= learning_rate*grads['dW'+str(l+1)]\n",
    "        parameters['b'+str(l+1)] -= learning_rate*grads['db'+str(l+1)]\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "# Check the functionality of the above function\n",
    "parameters = {'b2': np.array([[-0.84610769]]), 'W1': np.array([[-0.59562069, -0.09991781, -2.14584584,  1.82662008],[-1.76569676, -0.80627147,  0.51115557, -1.18258802],[-1.0535704 , -0.86128581,  0.68284052,  2.20374577]]), 'b1': np.array([[-0.04659241],[-1.28888275],[ 0.53405496]]), 'W2': np.array([[-0.55569196,  0.0354055 ,  1.32964895]])}\n",
    "grads = {'db1': np.array([[ 0.88131804],[ 1.70957306],[ 0.05003364]]), 'db2': np.array([[ 0.98236743]]), 'dW1': np.array([[ 1.78862847,  0.43650985,  0.09649747, -1.8634927 ],[-0.2773882 , -0.35475898, -0.08274148, -0.62700068],[-0.04381817, -0.47721803, -1.31386475,  0.88462238]]), 'dW2': np.array([[-0.40467741, -0.54535995, -1.54647732]])}\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:100%\"> \n",
    "    <tr>\n",
    "    <td > W1 </td> \n",
    "           <td > [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
    " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
    " [-1.0535704  -0.86128581  0.68284052  2.20374577]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > b1 </td> \n",
    "           <td > [[-0.04659241]\n",
    " [-1.28888275]\n",
    " [ 0.53405496]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > W2 </td> \n",
    "           <td > [[-0.55569196  0.0354055   1.32964895]]</td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > b2 </td> \n",
    "           <td > [[-0.84610769]] </td> \n",
    "  </tr> \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialization\n",
    "- Cost Computation\n",
    "- Forward Propagation\n",
    "- Backward Propagation\n",
    "- Model creation\n",
    "- Update parameters\n",
    "- Prediction\n",
    "- Gradient Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def final_model(X,y,layer_dims,learning_rate=0.0075,iterations=3000,print_cost=False):\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    \n",
    "    parameters = initialize_params(layer_dims)\n",
    "#     print(parameters.keys())\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        # Forward propagation step\n",
    "        AL, caches = model_forward(X,parameters)\n",
    "        \n",
    "        # Cost computation step\n",
    "        cost = compute_cost(AL,y)\n",
    "        \n",
    "        # Back propagation step\n",
    "        grads = model_backward(AL,y,caches)\n",
    "        \n",
    "        # Updating parameters\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "        \n",
    "        if print_cost and i%100 == 0:\n",
    "            print(\"Cost after %d iteration = %f\"%(i,cost))\n",
    "        if i%100 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50)\n"
     ]
    }
   ],
   "source": [
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 209\n",
      "Number of testing examples: 50\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_x_orig shape: (209, 64, 64, 3)\n",
      "train_y shape: (1, 209)\n",
      "test_x_orig shape: (50, 64, 64, 3)\n",
      "test_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "# Explore your dataset \n",
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (12288, 209)\n",
      "test_x's shape: (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layers_dims = [12288, 20, 7, 5, 1] #  5-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iteration = 0.771749\n",
      "Cost after 100 iteration = 0.672665\n",
      "Cost after 200 iteration = 0.649577\n",
      "Cost after 300 iteration = 0.610020\n",
      "Cost after 400 iteration = 0.554252\n",
      "Cost after 500 iteration = 0.507791\n",
      "Cost after 600 iteration = 0.448387\n",
      "Cost after 700 iteration = 0.390049\n",
      "Cost after 800 iteration = 0.352117\n",
      "Cost after 900 iteration = 0.302107\n",
      "Cost after 1000 iteration = 0.254408\n",
      "Cost after 1100 iteration = 0.215121\n",
      "Cost after 1200 iteration = 0.194182\n",
      "Cost after 1300 iteration = 0.170332\n",
      "Cost after 1400 iteration = 0.152721\n",
      "Cost after 1500 iteration = 0.137127\n",
      "Cost after 1600 iteration = 0.125351\n",
      "Cost after 1700 iteration = 0.115262\n",
      "Cost after 1800 iteration = 0.105798\n",
      "Cost after 1900 iteration = 0.098460\n",
      "Cost after 2000 iteration = 0.094307\n",
      "Cost after 2100 iteration = 0.085911\n",
      "Cost after 2200 iteration = 0.081947\n",
      "Cost after 2300 iteration = 0.076542\n",
      "Cost after 2400 iteration = 0.071785\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdAAAAGHCAYAAAAA1gNPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTngx2Sl+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ffb996b09b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = final_model(train_x, train_y, layer_dims = layers_dims, iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X,y,parameters):\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "    \n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
